# Getting started

## `dso init` -- Initialize a project

`dso init` initializes a new project in your current directory. In the context of DSO, a project is a structured environment where data science workflows are organized and managed.

To initialize a project use the following command:

```bash
# To initialize a project called "test_project" use the following command
dso init test_project --description "This is a test project"
```

It creates the root directory of your project with all the necessary configuration files for `git`, `dvc`, `uv` and `dso` itself.

## `dso create` -- Add folders or stages to your project

We consider a _stage_ an individual step in your analysis, usually a script with defined inputs and outputs.
Stages can be organized in _folders_ with arbitrary structures. `dso create` initializes folders and stages
from predefined templates. We recommend naming stages with a numeric prefix, e.g. `01_` to declare the
order of scripts, but this is not a requirement. Currently, two stage templates have been implemented that
use either a quarto document or bash script to conduct the analysis.

```bash
cd test_project

# Let's create a folder that we'll use to organize all analysis steps related to "RNA-seq"
dso create folder RNA_seq

# Let's create first stage for pre-processing
cd RNA_seq
dso create stage 01_preprocessing --template bash --description "Run nf-core/rnaseq"

# Let's create a second stage for quality control
dso create stage 02_qc --template quarto --description "Perform RNA-seq quality control"
```

Stages have the following pre-defined folder-structure. This folder system aims to make the structure coherent throughout a project for easy readability and navigation. Additional folders can still be added if necessary.

```text
stage
  |-- input            # contains Input Data
  |-- src              # contains Analysis Script(s)
  |-- output           # contains TLF - Outputs generated by Analysis Scripts
  |-- report           # contains HTML Report generated by Analysis Scripts
```

## Configuration files

The config files in a _project_, _folder_, or _stage_ are the cornerstone of any reproducible analysis, serving as a single point of truth. Additionally, using config files reduces the modification time needed for making _project_/_folder_-wide changes.

Config files are designed to contain all necessary parameters, input, and output files that should be consistent across the analyses. For this purpose, configurations can be defined at each level of your project in a `params.in.yaml` file. These configurations are then transferred into the `params.yaml` files when using `dso compile-config`.

A `params.yaml` file consolidates configurations from `params.in.yaml` files located in its parent directories, as well as from the `params.in.yaml` file in its own directory. For your analysis, reading in the `params.yaml` of the respective stage gives you then access to all the configurations.

The following diagram displays the inheritance of configurations:

```{eval-rst}
.. image:: ../img/dso-yaml-inherit.png
   :width: 60%
```

### Writing configuration files

To define your configurations in the `params.in.yaml` files, please adhere to the yaml syntax. Due to the implemented configuration inheritance, relative paths need to be resolved within each **folder** or **stage**. Therefore, relative paths need to be specified with `!path`.

An example `params.in.yaml` can look as follows:

```bash
thresholds:
  fc: 2
  p_value: 0.05
  p_adjusted: 0.1

samplesheet: !path "01_preprocessing/input/samplesheet.txt"

metadata_file: !path "metadata/metadata.csv"

file_with_abs_path: "/data/home/user/typical_analysis_data_set.csv"

remove_outliers: true

exclude_samples:
  - sample_1
  - sample_2
  - sample_6
  - sample_42
```

### Compiling `params.yaml` files

All `params.yaml` files are automatically generated using:

```bash
dso compile-config
```

### Overwriting Parameters

When multiple `params.in.yaml` files (such as those at the project, folder, or stage level) contain the same configuration, the value specified at the more specific level (e.g., stage) takes precedence over the value set at the broader level (e.g., project). This makes the analysis adaptable and enhances modifiability across the project.

## Implementing a stage

A stage is a single step in your analysis and usually generates some kind of output data from input data. The input data can also be supplied by previous stages. To create a stage, use the `dso create stage` command and select either the _bash_ or _quarto_ template as a starting-point.

The essential files of a stage are:

-   `dvc.yaml`: The DVC configuration file that defines your data pipelines, dependencies, and outputs.
-   `params.yaml`: Auto-generated configuration file.
-   `params.in.yaml`: Modifiable configuration file containing stage-specific configurations.
-   `src/<stage_name>.qmd`(optional): A Quarto file containing your script that runs the analysis for this stage.

### dvc.yaml

The `dvc.yaml` file contains information about the parameters, inputs, outputs, and commands used and executes in your stage.

#### Configuring the `dvc.yaml`

Configurations stored in the `params.yaml` of a stage can be directly used within the `dvc.yaml`:

```bash
stages:
  01_preprocessing:
    # Parameters used in this stage, defined in params.yaml
    params:
      - dso
      - thresholds
    # Dependencies required for this stage, can be defined in the params.yaml (define with ${...})
    deps:
      - src/01_preprocessing.qmd
      - ${file_with_abs_path}
      - ${samplesheet}

    # Outputs generated by this stage
    outs:
      - output
      - report/01_preprocessing.html
```

### Quarto Stage

By default, a Quarto stage includes the following cmd in the `dvc.yaml` file:

```
    # Command to render the Quarto script and move the HTML report to the report folder
    cmd:
      - dso exec quarto .
```

### Bash Stage

A Bash stage, by default, does not include an additional script. Bash code can be directly embedded in the `dvc.yaml` file:

```
    cmd:
      - |
        bash -euo pipefail << EOF

        # add bash code here

        EOF
```

### Accessing Files and Configurations with R and Python

You can easily access files and configurations using either the DSO R-package or the Python module.

A convenient way of accessing files and configurations of your is to use the DSO R-package or the Python module.

For Python, refer to the [Python Usage Page](python_usage.md).

For R, refer to the [R Package Page](https://boehringer-ingelheim.github.io/dso-r/).

## `dso repro` -- Reproducing all stages

To execute or reproduce a stage, folder, or project use `dso repro`. `dso repro` is a wrapper around `dvc repro` and builds the config files before reproducing the complete or a part of the analyses pipeline.

Several command options are available and are detailed in the [dvc repro documentation](https://dvc.org/doc/command-reference/repro). The most common usages are detailed below:

```bash
# Reproducing the whole project
dso repro

# Reproducing all stages within a specific directory
dso repro -R <path>

# Reproducing a single stage with its dependency stages
dso repro subfolder/my_stage/dvc.yaml

# Reproducing a single stage without its dependency stages
dso repro -s subfolder/my_stage/dvc.yaml

# Reproduce stage even if no changes were found
dso repro -s -f subfolder/my_stage/dvc.yaml
```

## Syncing Changes with Remote

To ensure your data and code are synchronized with the remote storage and repository, follow these steps:

### Add a Remote Data Storage

To ensure you can always revert to previous data versions, add a remote storage for your DVC-controlled data. Use the `dvc remote add` command to specify a remote directory where the version-controlled files will be stored.

We recommend creating a directory in a suitable long-term storage location. Use the `-d` (default) option of `dvc remote add` to set this directory as the default remote storage:

```bash
# Create a directory for storing version-controlled files
mkdir /long/term/storage/project1/DVC_STORAGE

# Execute within the project directory to define the remote storage
dvc remote add -d <remote_name> /long/term/storage/project1/DVC_STORAGE
```

### Track Data with DVC

In your DSO project, all outputs are automatically controlled by DVC, ensuring that your data is versioned and managed efficiently. This setup helps maintain reproducibility and consistency across your analysis.

When you have input data that was not generated within your pipeline, you need to add them to your DSO project. Use `dvc add` to track such files with DVC. This command creates an associated `.dvc` file and automatically appends the tracked file to `.gitignore`. The `.dvc` file acts as a placeholder for the original file and should be tracked by Git.

This command is particularly useful when data is generated outside of your DSO project but is used within your analysis, such as metadata or preprocessed data.

```bash
# Add a file to DVC
dvc add <directoryname/filename>

# Example Usage:
dvc add metadata/external_clinical_annotation.csv
```

### Push Changes to Remote

After tracking your data with DVC and committing your changes locally, you need to push these changes to both the remote storage and your Git repository. This ensures that your data and metadata are safely backed up and accessible to collaborators.

Hereâ€™s how to do it:

```bash
# Push DVC-controlled data to the remote storage
dvc push

# Commit changes to the Git repository with a descriptive message
git commit -m "Descriptive commit message"

# Push committed changes to the remote Git repository
git push
```
